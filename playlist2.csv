Module;Link;Transcript;;;;
7. Creating a Free Gemini AI API Token;https://epam.udemy.com/assets/65239373/files/2025-05-19_13-24-39-e9b0432abed617654f061458eba3c9a6/2/aa00ec69985284ee7aa555e8a3d2c689505c.m3u8?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiMjAyNS0wNS0xOV8xMy0yNC0zOS1lOWIwNDMyYWJlZDYxNzY1NGYwNjE0NThlYmEzYzlhNi8yLyIsImV4cCI6MTc1NTI4OTIzNH0.R0JMG9VJNNioGiDic0xrOa86hERGpmdJgemCBuC4SuM&provider=cloudfront&v=1;"I'm super excited to discuss this piece of the course with you, because I believe that if you understand
this specific video, then you're gonna have a very, very good mental model about how agents work and
what you can do with them.
So I hope you will like it and that you will understand it, because this will open us the mental way
to think about how agents work and how MSPs work.
Now here is the thing.
So if you think about how you interact with a chat UI, an LLM like ChatGPT or Claude or or Gemini,
then usually your interaction is very easy.
You send a chat message, you receive a response, you chat, you send a follow up message, and again
you receive a response.
That's super simple, right?
Of course you might do some other things like attach an image or upload some documents, or include
work in projects or upload assets whatsoever.
But this is the basic workflow.
But there is so much more that happens behind the scenes, behind the scenes.
This is very much what happens here.
Are you.
And then there is an lm API so that the UI but an API.
And if you communicate directly to the API, then you must provide much more context with each call,
as if you work with the ChatGPT UI, for example, because ChatGPT and all the others do this for you
automatically.
Now what happens here is that you send a so-called message request, but you can think about it as a
as a message where you send not only your York current message.
But you also send the system prompt.
So some instructions about what to expect from the model to do.
This is usually optional.
Then you also send the chat history.
So by default LMS if you communicate through the API they don't remember the chat history.
You need to send a whole conversation in each request and keep track of this conversation yourself on
the client side.
And then you might send a bunch of other things.
That's why you see this here in braces.
You can send, for example, the maximum number of output tokens that you want to pay for.
So for example, you can instruct ChatGPT or Cloud or Gemini to to cut off the message when it hits
a certain number of tokens.
Like think about it like cutting off the message when it hits like 20 words in the response, because
you don't want to pay for more.
And then you also send a message.
Right?
So your content the payload itself.
So this is what happens.
And then you get a response.
And in the response you receive things like the message itself or the response message.
This is what you would see on the ChatGPT UI two.
And you also have depending on the on the LLM provider, you might have a stop reason.
The stop reason might be simply stop, which means it's all good.
You get all the response, but the stop reason can be for example, stopping generation because it would
have generated offensive content or it would have leaked personally identifiable information whatsoever.
So you can also inspect the stop reason.
You will see how many tokens you used.
And also in with some providers you can see the price of your request like how much you paid for for
this specific request.
And of course, as you probably know, you can go to Anthropic's website or OpenAI's website and just
sign up for an API key, upload your balance with a a few tens of dollars, and then start using these
platforms through the API right away.
Okay.
And this goes on like that, right?
You have a follow up request and then you have a response.
Now here is one thing that many people don't know about.
And this will be the most important piece in the whole course I would say.
And this is called the LLM tool use.
Now what happens is that you not only can send messages, and the LLM backend not only can respond with
a message, but there is also a meta communication layer between you and the LM.
So for example, if you have a function defined in your Python environment which can get the current
price of a crypto symbol, let's say Bitcoin, then what you can do is that you can extend the LMS functionality
by providing this function to the LM, and it's not like it doesn't go that way, that you send this
function to the LM and it will execute it, uh, in their own infrastructure.
But it goes in a way that you implement your function for yourself in your Python file or Jupyter notebook.
And then when you send the message request, then you also add a metadata, a so-called tool use or
function calling metadata where you say, oh, and I also have this function.
Now this function is called get crypto price.
It accepts one parameter called symbol which is the crypto symbol, and it gets the current price of
the cryptocurrency.
This you can define in a metadata and just send it to the LM.
Now the LM takes note of it.
It says oh cool, I got it.
You have a function like that.
And then you can keep on working, keep on working.
And at one point you might get into a situation where in order to the LM to generate a meaningful response,
it would need the current price of a cryptocurrency.
And then what it will do is that in its response, it won't come back to you with a message, but it
will use an other part of the response, like telling you kind of on a meta level that, Hey, before
I get back to you with a message, can you please call your function that you told me you have the get
crypto price function with this and that parameters, and then you can go execute the function on your
computer and then send the follow up request.
The follow up request in this case can be just a very standard LM message which says for example, sure,
the current price of Bitcoin is this and that, and then the LM will go on and for example, write a
haiku about the current price of Bitcoin, if that was the original request.
So this is the most important piece of agents that they can expose functionality to each other.
So you can expose functionality to an LM.
Now think about that.
Probably an even better use case would be to provide web search functionality to an LLM.
Now, most of the LMS can do that.
At least most of the Hastie LMS managed LMS can do that automatically, but you could simply just connect,
for example, to perplexity, sign up for an API key and then write a function that can do a perplexity,
search and return with perplexity.
Answer to the LLM.
And in this case you would be able to tell the LLM.
Hey, I have this function.
It's called perplexity search.
It takes a keyword argument and returns with a bunch of jsons about the, uh, the response.
So just let me know when you need to search the web, and then the LLM will reach out to you in a response
whenever it needs external information.
So this is the big thing with LMS.
And this is what enables cooperative agents.
I see you in the next video.";;;;
8. Agentic AI Behavior and Tool Calling Hands on;https://epam.udemy.com/assets/65239465/files/2025-05-19_13-24-41-26d4c2ee6c417e17348e14cb53f88e19/2/aa00c827e8951ae5642f85e927e0ad174da2.m3u8?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiMjAyNS0wNS0xOV8xMy0yNC00MS0yNmQ0YzJlZTZjNDE3ZTE3MzQ4ZTE0Y2I1M2Y4OGUxOS8yLyIsImV4cCI6MTc1NTI5MTI5Mn0.jNrTTiHGqRm2ct914NUmtOMDq-4Rdz7cMsSTe15jNA4&provider=cloudfront&v=1;"Now that you have added the Gemini API key to the env file, let us validate that it's actually there.
So we are going to use the load env function from the Python library that manages this file, and reads
everything from this file into the system environment.
Then we're going to go and just check if the Gemini API key is there.
And then if it's there, we create a Google Gen AI model.
And we're going to use here Gemini to flash.
Everything that we use here has been already imported here.
Right.
Like the gen AI model and the load function.
Let me execute it.
It works well and it should work well for you too.
So it says the Gemini model loaded successfully.
And this is the model that.
I've loaded.
Now let's go and chat a little bit with our Gemini backend.
So let's send it the message.
What is the current price of Bitcoin?
The way to chat with Gemini in a native way is to start a chat with the chat, and then send a chat
message and just read out the text piece of the response.
All right, so this is the simplest way to go.
Now I'm executing it by pressing Ctrl enter.
So here we go.
It says unfortunately I cannot provide you with exact real time price of bitcoin because it fluctuates.
You should go and take a look at these exchanges.
Now one of the exchanges it suggests us to use is Binance, which is one of the largest crypto exchange.
So let's implement this functionality that Gemini on our computer is able to reach out to Binance and
pull in the current crypto price.
Okay, here is a simple implementation.
But before we get there, I also want to show you the full response here.
We put the response text to the screen.
And now let's see what we've actually what we actually got from Gemini.
So this is the response uh low level.
So it says um this is the response.
We are done.
The response itself has well, a single candidate.
And this candidate, this is our response.
It says it has a part and it has a tax base.
And here comes the unfortunate.
I'll provide you with the exact real time price of Bitcoin.
So the actual text.
Right.
So this is how we receive the response from the model with a bunch of metadata.
So it says that this is what the model said.
And the reason is setup as we discussed briefly the reason can be anything else.
Like if you specify a maximum token count, you can run out of tokens.
If the LLM would generate a swear word, then it might just stop right there.
So this would pop up in the finish.
Reason.
And there are a bunch of other metadata about uh, the also the input like how many tokens you use and
also the output.
All right.
So this is what we actually receive and we extract the text from that in such a simple example.
So let us move on and add this, uh, Bitcoin functionality.
Now I'm certainly not an expert on uh, on bitcoin and, and crypto, but what I know is that if you
want to take a look at the USD price of Bitcoin, at least on Binance, then the simplest you can do
through the API is to compare the Bitcoin price to a stablecoin, which is called USDt.
So we are going to use here USDt.
If you are not a crypto expert either either then just take it as is okay.
So this approximates the USD price of Bitcoin.
So if you take a look at the Binance API, you will see that this is the exact code that you can do
to get the current price of Bitcoin.
We're going to use the request library which is a probably the most popular Python library for accessing
APIs and websites.
We just pull in this URL and then take the responses JSON and print it to the screen.
Let's take a look.
Okay, so this is what Binance tells us.
It says for this specific pair.
So this specific symbol the price is now this value okay.
So now we have an LLM.
We know how to get a Bitcoin price.
So we just need to glue everything together right.
So let's create a function that does exactly the same.
It extracts a certain symbol price from Binance.
This is what you've already seen.
It just parameterized now with a symbol.
And we extract the price itself from the response JSON because that was a dictionary, right?
So let's define this function.
That's nothing fancy, it's just a Python function.
Let's use it with the btc USDt symbol.
All right.
It says this is the BTC price.
We converted everything into a float.
That's why we don't have that many digits at the end.
So now we have a function that we can pass to to the LM.
So let's go.
Let's implement this.
Now what we're going to do is to create kind of a metadata definition that we can pass to Gemini and
introduce Gemini to our Bitcoin currency, uh function.
Okay.
So the way to go and this is Gemini specific.
This is just how to do it.
If you take a look at the Gemini documentation, you will see that like that's the way to do it.
You can define function declarations and send it to Gemini.
I will say I have a function which is called get crypto price.
The description is get cryptocurrency price in USDt from Binance.
It is important what you write in the description, because the LLM will need to decide when and if
it needs to use a certain tool, a certain function.
It will only use it if you if the discussion is about crypto price.
But it's important that you, uh, you add a meaningful description here so it knows what it is about.
Then we have a bunch of parameters.
I mean, we actually have a single parameter which is the symbol, right?
The type is a string.
And then here is a description.
It says this is a cryptocurrency trading pair symbol.
It can be BTC, USDt or ETH Th USDt for Ethereum.
And I also give it some hints because we cannot really rely on Gemini knowing how to use Binance.
I mean, it might have a notion about how Binance API work because it can very easily be part of their
Gemini training data, like all the dump from the Binance documentation.
But it's better to be safe than sorry, right?
So I would just say just for you to know, the symbol for Bitcoin is BTC, USDt, and the symbol for
Ethereum is ETH, USDt.
Okay.
And we also pass that there is one required symbol and one required parameter.
And that is the symbol a bit of a complex definition.
But that's what we need to inject into Gemini to get started.
I would also like to draw your attention to the fact that we never mentioned our function here.
So we never actually used the get crypto price function object by chance.
Now the name of our function declaration is Get crypto price, but it has technically nothing to do
with this function.
We are just talking metadata here, so I'm just telling Gemini that, hey, I have a function and I
want you to refer to this function as crypto price.
And this is how this function works.
But Gemini will won't it won't be able to directly call your function in this case okay.
So it's all just metadata definition.
So let's execute it.
Of course it gets executed because it's just a standard Python dictionary.
And then now I'm going to start a new chat and send the message the same message saying what is the
current price of Bitcoin?
But I'm also on top of the message.
I'm also sending a tool definition.
All right.
So I will tell Gemini that I also have access to tools.
So if you want to me to use tools just let me know.
So let's go and execute this.
This is going to be exciting.
All right.
So here is the response.
You see the response is very much the same.
Then it's true.
And here is the response whatsoever.
But here now if you take a look at the parts here you see that.
Now we don't have a text response but we have a function called response.
And Gemini tells us, hey, I don't want to give you any text answer yet.
Can you just go and call your get crypto price function with the argument BTC USD and send me back the
results.
And that's very much it.
So let's do that.
Let's execute the get crypto price function with the symbol.
Gemini wanted us to execute it with extract the price.
And once we get the price, let's just send it back as is.
So Gemini just adds this, you know, call this function.
And what it wants us to provide it with is the result of this function.
So I'm just going to convert it into string and send it back execute it.
And then here we go.
There is a new message appearing.
And here is the response.
So it says now I can send you a text and the text is okay.
Thanks for confirming the current price of Bitcoin.
Is this and that.
Okay.
So now if I'm just taking a look at the final response is the text and I can pop this up on the screen.
And now I have an Agentic Gemini that is not only able to talk with me, but also use tools.
Tools that I provide it with.
And with the help of my tool, now it will be able to extract the and get the price of of Bitcoin.
Now this is the trick.
And if you understand what happened here, then I believe you understand very much everything that enables
agent to be so powerful.
So we can use Llms and provide them with functions and then execute these functions on their behalf
on our computer.
What you've seen here, this was now very, very low level so to say.
So I don't think too many People interact with Llms on such a low level.
There are frameworks with, for example, long chain, with the help of which you will be able to just
define a function and add it to the LLM.
And the long chain will be able to do this whole back and forth.
Right.
So if there is a tool call, it will actually call the function and return the result.
And what you will see is just the end result.
You ask for Bitcoin with the two registries and you will see this end result.
For example, if you use long chain or or another agent library.
So there is like lots and lots of uh, abstraction on top in modern systems.
But I wanted to ensure that you understand the kind of the bottom of how it works.
So you have the best possible mental model as we are moving on, so you can understand fully how mcps-prs
work.
So congrats, and I'll see you in the next video.";;;;
